# -*- coding: utf-8 -*-
"""SA_XGC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fmA0Rc1jpH5DRMD2TvgpZkTQmgeCgR2n

# **Modelling MBTI with XGBoost**
###Visualizing with WordCloud
"""

from google.colab import files
uploaded = files.upload()

import io
import pandas as pd

df = pd.read_csv(io.BytesIO(uploaded['mbti_1.csv']))
print(uploaded.keys())

df.head() #Performing minor EDA

df.info()

df.isna().sum()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

cnt_types = df['type'].value_counts()

plt.figure(figsize=(15,5))
sns.barplot(cnt_types.index, cnt_types.values, alpha=0.8,palette='Dark2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Personality types', fontsize=12)
sns.set()
plt.show()

def get_types(row):
    t=row['type']

    I = 0; N = 0
    T = 0; J = 0
    
    if t[0] == 'I': I = 1
    elif t[0] == 'E': I = 0
    else: print('I-E incorrect')
        
    if t[1] == 'N': N = 1
    elif t[1] == 'S': N = 0
    else: print('N-S incorrect')
        
    if t[2] == 'T': T = 1
    elif t[2] == 'F': T = 0
    else: print('T-F incorrect')
        
    if t[3] == 'J': J = 1
    elif t[3] == 'P': J = 0
    else: print('J-P incorrect')
    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) 

df = df.join(df.apply (lambda row: get_types (row),axis=1))

df.head()

#Separately counting each and every data type
print ("Introversion (I) /  Extroversion (E):\t", df['IE'].value_counts()[0], " / ", df['IE'].value_counts()[1])

print ("Intuition (N) / Sensing (S):\t\t", df['NS'].value_counts()[0], " / ", df['NS'].value_counts()[1])

print ("Thinking (T) / Feeling (F):\t\t", df['TF'].value_counts()[0], " / ", df['TF'].value_counts()[1])

print ("Judging (J) / Perceiving (P):\t\t", df['JP'].value_counts()[0], " / ", df['JP'].value_counts()[1])

#Printing the correlation matrix
##This helps us check the linear relationship between the pairs of persona traits
corr = df[['IE','NS','TF','JP']].corr()
print(corr)

plt.figure(figsize=(10,10))
plt.title('Pearson Features Correlation', size=15)
sns.heatmap(corr, cmap="coolwarm",  annot=True, linewidths=1)

import numpy as np
b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}
b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]

def translate_personality(personality):
    #MBTI to binary vector
    return [b_Pers[l] for l in personality]

def translate_back(personality):
    #binary vector to MBTI personality
    
    s = ""
    for i, l in enumerate(personality):
        s += b_Pers_list[i][l]
    return s

#Checking
d = df.head(4)
list_personality_bin = np.array([translate_personality(p) for p in d.type])
print("Binarize MBTI list: \n%s" % list_personality_bin)

import nltk
nltk.download("popular")

"""**Text Preprocessing using NLTK**"""

from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords 
from nltk import word_tokenize


unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',
       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']
  
unique_type_list = [x.lower() for x in unique_type_list]

#Lemmatize
stemmer = PorterStemmer()
lemmatiser = WordNetLemmatizer()

#Removing stopwords
cachedStopWords = stopwords.words("english")

import re

def pre_process_data(df, remove_stop_words=True, remove_mbti_profiles=True):

    list_personality = []
    list_posts = []
    len_data = len(df)
    i=0
    
    for row in df.iterrows():
        i+=1
        if (i % 500 == 0 or i == 1 or i == len_data):
            print("%s of %s rows" % (i, len_data))

        ##Removing stop-words, cleaning comments, lowercasing all characters
        posts = row[1].posts
        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)
        temp = re.sub("[^a-zA-Z]", " ", temp)
        temp = re.sub(' +', ' ', temp).lower()
        if remove_stop_words:
            temp = " ".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])
        else:
            temp = " ".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])
            
        if remove_mbti_profiles:
            for t in unique_type_list:
                temp = temp.replace(t,"")

        type_labelized = translate_personality(row[1].type)
        list_personality.append(type_labelized)
        list_posts.append(temp)

    list_posts = np.array(list_posts)
    list_personality = np.array(list_personality)
    return list_posts, list_personality

list_posts, list_personality  = pre_process_data(df, remove_stop_words=True)

"""**Text Data Vectorization using CountVectorizer and TFIDF**"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.manifold import TSNE

# Posts to a matrix of token counts
cnt = CountVectorizer(analyzer="word", 
                             max_features=1500, 
                             tokenizer=None,    
                             preprocessor=None, 
                             stop_words=None,  
                             max_df=0.7,
                             min_df=0.1) 

# Learning the vocabulary dictionary and return term-document matrix
print("Tokenizing words using CountVectorizer...")
X_cnt = cnt.fit_transform(list_posts)

# Transforming the count matrix to a normalized tf or tf-idf representation
tf = TfidfTransformer()

print("Measuring their statistical relevance using TF-IDF...")
# Learning the idf vector (fit) and transform a count matrix to a tf-idf representation
X_tfidf =  tf.fit_transform(X_cnt).toarray()

feature_names = list(enumerate(cnt.get_feature_names()))

#Checking the dimensions of the feature vector
X_tfidf.shape

"""**Performing XGBoost Classification**"""

type_indicators = [ "IE: Introversion (I) / Extroversion (E)", "NS: Intuition (N) – Sensing (S)", 
                   "FT: Feeling (F) - Thinking (T)", "JP: Judging (J) – Perceiving (P)"  ]

print("\t\t Printing the four major dimensions of personality classification in MBTI:")
for l in range(len(type_indicators)):
    print("\t\t\t\t\t"+type_indicators[l])

print("MBTI 1st row: %s" % translate_back(list_personality[0,:]))
print("Y: Binarized MBTI 1st row: %s" % list_personality[0,:])

from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X = X_tfidf

# Training type indicator individually
for l in range(len(type_indicators)):
    print("%s ..." % (type_indicators[l]))

   #Selecting individual personality targets
    Y = list_personality[:,l]

    #Splitting our data
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=21)

    #Fitting our model
    model = XGBClassifier()
    model.fit(X_train, y_train)
    print("...\nTraining complete")

    #Predicting on our test set
    y_pred = model.predict(X_test)
    predictions = [round(value) for value in y_pred]

    #Calculating accuracy
    accuracy = accuracy_score(y_test, predictions)
    print("* %s Accuracy: %.2f%%" % (type_indicators[l], accuracy * 100.0))

#Checking the default values of the XGBoost Algo
default_get_xgb_params = model.get_xgb_params()
print (default_get_xgb_params)



"""**Adjusting parameters according to our datatset**"""

param = {}

param['n_estimators'] = 200
param['max_depth'] = 2
param['nthread'] = 8
param['learning_rate'] = 0.2

#Training type indicator individually
for l in range(len(type_indicators)):
    print("%s :\n\n" % (type_indicators[l]))
    
    Y = list_personality[:,l]

    #Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=21)

    #Fitting our model
    model = XGBClassifier(**param)
    model.fit(X_train, y_train)
    #Analysis on test set
    y_pred = model.predict(X_test)
    predictions = [round(value) for value in y_pred]
    #Calculating accuracy individually
    accuracy = accuracy_score(y_test, predictions)
    print("* %s Accuracy: %.2f%%" % (type_indicators[l], accuracy * 100.0))

#Experimenting with my own words
my_posts  = """Paradoxical, it is.

A child, a glimmer of continuity, regarded as one of the greatest gifts,
Yet we cannot absolve our expectations,
Dreaming endearingly for one, only to crush their dreams, so methodically?
"If we compare a fish by it's ability to climb a tree, it will spend it's entire life-
Believing, it is stupid."
Say,
Why indoctrinate, when you can celebrate creativity?

Let's teach our future how to think, for progress will be measured when we can encash the approaching demographic dividend.

Expand imagination, not syllabuses.

Happy Children's Day.
"""

mydata = pd.DataFrame(data={'type': ['ENTP'], 'posts': [my_posts]})

my_posts, dummy  = pre_process_data(mydata, remove_stop_words=True)

my_X_cnt = cnt.transform(my_posts)
my_X_tfidf =  tf.transform(my_X_cnt).toarray()

result = []
#Training type vector indivudually
for l in range(len(type_indicators)):
    print("%s : Training complete.\n " % (type_indicators[l]))
    
    Y = list_personality[:,l]

    #Train-Test-Split
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=21)

    #Fitting our model
    model = XGBClassifier(**param)
    model.fit(X_train, y_train)
    
    #Running the test sets
    y_pred = model.predict(my_X_tfidf)
    result.append(y_pred[0])

print("The result is: ", translate_back(result))

pip install Pillow

"""**Generating WordCloud**"""

from imageio import imread
from wordcloud import WordCloud, STOPWORDS

fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,10*len(df['type'].unique())))

k = 0
for i in df['type'].unique():
    df_4 = df[df['type'] == i]
    wordcloud = WordCloud().generate(df_4['posts'].to_string())
    ax[k].imshow(wordcloud)
    ax[k].set_title(i)
    ax[k].axis("off")
    k+=1

"""**Generating personalized WordCloud (Image Masking)**"""

brain = files.upload()

from imageio import imread
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from PIL import Image
brain_mask = np.array(Image.open("head3.jpg"))
fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,10*len(df['type'].unique())))

k = 0
for i in df['type'].unique():
    df_4 = df[df['type'] == i]
    wordcloud = WordCloud(background_color="white", max_words=2000,mask=brain_mask).generate(df_4['posts'].to_string())
    image_colors = ImageColorGenerator(brain_mask)
    ax[k].imshow(wordcloud)
    ax[k].set_title(i)
    ax[k].axis("off")
    k+=1

img=files.upload()

from imageio import imread
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from PIL import Image
brain_mask = np.array(Image.open("brain5.jpg"))
fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,10*len(df['type'].unique())))

k = 0
for i in df['type'].unique():
    df_4 = df[df['type'] == i]
    wordcloud = WordCloud(mask=brain_mask,background_color='white',contour_color='black',contour_width=0.01, colormap='plasma').generate(df_4['posts'].to_string())
    image_colors = ImageColorGenerator(brain_mask)
    ax[k].imshow(wordcloud)
    ax[k].set_title(i)
    ax[k].axis("off")
    k+=1